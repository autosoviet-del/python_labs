def normalize(text: str, *, casefold: bool = True, yo2e: bool = True):
    """Функция для нормализации текста."""
    s = text                     # Копируем исходный текст в переменную s

    # 1. Приведение текста к нормализованному регистру (регистр нечувствительный)
    if casefold:                  # Если разрешено применять case folding
        s = s.casefold()          # Приводим весь текст к нормализованному регистру (case folding)
                                  # Это лучший способ привести текст к общему формату,
                                  # обеспечивающему сравнение без учета регистров символов

    # 2. Замена буквы «ё» на «е»
    if yo2e:                      # Если разрешена замена буквы «ё»
        s = s.replace('ё', 'е')   # Меняем маленькие буквы «ё» на «е»
        s = s.replace('Ё', 'Е')   # Меняем большие буквы «Ё» на «Е»
                                  # Часто возникает необходимость упростить текст,
                                  # заменяя менее распространённую букву «ё» на «е»

    # 3. Удаление непечатаемых символов (управляющих символов)
    s = s.replace('\t', ' ')      # Табуляции (\t) заменяем на обычные пробелы
    s = s.replace('\r', ' ')      # Перенос каретки (\r) также заменяем на пробелы
    s = s.replace('\n', ' ')      # Новую строку (\n) тоже меняем на пробел
                                  # Теперь все переходы на новую строку, табуляции и возвраты каретки стали пробелами

    # 4. Стандартизация пробелов
    s = ' '.join(s.split())       # Преобразовываем строку таким образом, чтобы оставались только единичные пробелы
                                  # split() разбивает строку по пробелам, join() объединяет части снова с единственным пробелом
                                  # Так получается компактная версия текста без множества лишних пробелов

    return s                      # Возвращаем нормализованный текст

def tokenize(text):
    """Функция для выделения отдельных слов (токенов) из текста."""
    tokens = []                   # Список для хранения выделенных токенов
    word = ''                     # Буфер для временного накопления текущего слова

    for ch in text:               # Цикл перебора каждого символа в тексте
        if ch.isalnum() or ch == '_':  # Если символ — буква, цифра или подчёркивание
            word += ch            # Добавляем символ к текущему слову
        elif ch == '-' and word:   # Если встретили дефис, и он идёт после другого символа
            word += '-'           # Присоединяем дефис к слову
        else:                     # Во всех остальных случаях (символ не относится к словам)
            if word:              # Если накопилось какое-то слово
                tokens.append(word)# Сохраняем его в списке токенов
                word = ''         # Сбрасываем буфер слова
    if word:                      # После окончания цикла проверяем остаток
        tokens.append(word)       # Если осталась часть слова, добавляем её в список токенов

    return tokens                 # Возвращаем список полученных токенов

def top_n(freq, n=5):
    """Функция для выборки n наиболее частотных слов."""
    # Сортируем элементы словаря по следующим правилам:
    # 1. Первичный критерий: по количеству вхождения слова (частота, пара[1])
    # 2. Вторичный критерий: лексикографическое сравнение самих слов (пара[0])
    result = sorted(freq.items(), key=lambda pair: (-pair[1], pair[0]))
                                # Отрицательное значение частоты (pair[1]) означает сортировку по убыванию
                                # Второе значение (pair[0]) используется для дополнительной сортировки
                                # если частота совпадает

    return result[:n]            # Возвращаем только первые n элементов

